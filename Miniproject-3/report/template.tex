\newif\ifshowsolutions
\showsolutionstrue
\input{preamble}
\newcommand{\boldline}[1]{\underline{\textbf{#1}}}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 3\hfill
      March $11^{th}$, 2017 \\
    }
  }
}

\begin{document}
\pagestyle{fancy}

% LaTeX is simple if you have a good template to work with! To use this document, simply fill in your text where we have indicated. To write mathematical notation in a fancy style, just write the notation inside enclosing $dollar signs$.

% For example:
% $y = x^2 + 2x + 1$

% For help with LaTeX, please feel free to see a TA!



\section{Introduction}
\medskip
\begin{itemize}

    \item \boldline{Group members} \\
    Bolton Bailey and David Inglis

    \item \boldline{Team name} \\
    One Hot Team

    \item \boldline{Division of labour} \\
    We both pair coded the  visualization generators and discussed how to approach the project together.

\end{itemize}



\section{Basic Visualizations}
\medskip
\begin{itemize}

    \item \boldline{Justify your choice of visualization method}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'. Models and techniques should be bolded using '\textbf{}'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{What did you observe?}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{Did the results match what you would expect to see?}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{How do the ratings of the best movies compare to those of those of the most popular movies}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{How do the ratings of the three genres you chose compare to one another}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

    \item \boldline{Any other comparisons/observations}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Bullet:} Bullet text.
    \end{itemize}

\end{itemize}



\section{Matrix Factorization Algorithm}
\medskip
\begin{itemize}

    \item \boldline{Overall Implementation Architecture}
    We used the TA-provided HW6 solution for the matrix factorization portion. Once
    we had our factored matrix, we used numpy's implementation of single value decomposition
    to reduce the dimensionality of the data down to 2 dimensions. 
    
    \item \boldline{What parameters did you adjust and how?} \\

    \begin{itemize}
    \item \textbf{Stopping Criteria:} The default TA implementation of matrix factorization performs stochastic 
    gradient descent until the difference in mean-squared error between two 
    consecutive epochs is less than some $\epsilon$ fraction of the initial
    decrease in error from the start state to the first epoch. We experimented
    with varying values for this epsilon, ranging from $10e-4$ by several factors of
    $10$ in either direction, but found that unless we chose a very
    large $\epsilon$ the model converged to pretty much the same thing every time,
    so it didn't really matter what we chose. Using the default value, $\epsilon = 10e-4$,
    there was not much variation in results from run to run, so we decided to stick
    with that.

    \item \textbf{Step Size:} The step size shouldn't really affect what the
    gradient descent converges to, unless it is so big that it causes the descent
    to oscillate and diverge. The only disadvantage to choosing a very small 
    $\eta$ is that the SGD procedure will take longer to converge. We chose
    $\eta = 0.03$ which converged in a reasonable number of iterations, while
    providing the same level of accuracy as smaller $\eta$ values.

    \item \textbf{Regularization Parameter:} This was the value we experimented
    the most with. A larger $\lambda$ value will regularize the data more and bias
    the output matrices towards simpler models, while a lower $\lambda$ value leads
    to a more complex, unconstrained solution. We experimented with the following
    $\lambda$ values: $0$, $0.001$, $0.01$, $0.1$, and $1$. For the same set of
    movies, $\lambda = 0.001$ provided the most easily interpreted results, so
    we chose to use that $\lambda$ value.
    \end{itemize}

    \item \boldline{Did you make any other significant modifications or additions?} \\

    Besides the parameter tuning mentioned above we stuck to the stock TA and numpy
    implementations of matrix factorization and single value decomposition.
\end{itemize}



\section{Matrix Factorization Visualization}
\medskip
\begin{itemize}

    \item \boldline{What did you observe?} \\
    % Insert text here.

    \item \boldline{How do the ratings of the best movies compare to those of the most popular movies?} \\
    % Insert text here.

    \item \boldline{How do the ratings of the three genres you chose compare to one another?} \\

    \item \boldline{What was expected and what was surprising from the visualizations?} \\

    \item \boldline{Any other comparisons/observations?} \\

\end{itemize}



\section{Conclusion}
\medskip
\begin{itemize}

    \item \boldline{Briefly summarize your main observations} \\
    % Insert text here.

    \item \boldline{Did your visualizations help you to better understand the MovieLens dataset?} \\
    % Insert text here.

\end{itemize}



\end{document}
