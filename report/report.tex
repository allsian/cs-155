\newif\ifshowsolutions
\showsolutionstrue
\input{preamble}
\newcommand{\boldline}[1]{\underline{\textbf{#1}}}

\chead{%
  {\vbox{%
      \vspace{2mm}
      \large
      Machine Learning \& Data Mining \hfill
      Caltech CS/CNS/EE 155 \hfill \\[1pt]
      Miniproject 1\hfill
      Released January $28^{th}$, 2017 \\
    }
  }
}

\begin{document}
\pagestyle{fancy}

% LaTeX is simple if you have a good template to work with! To use this document, simply fill in your text where we have indicated. To write mathematical notation in a fancy style, just write the notation inside enclosing $dollar signs$.

% For example:
% $y = x^2 + 2x + 1$

% For help with LaTeX, please feel free to see a TA!



\section{Introduction}
\medskip
\begin{itemize}

    \item \boldline{Group members} \\
    Bolton Bailey and David Inglis

    \item \boldline{Team name} \\
    OneHotTeam

    \item \boldline{Division of labour} \\
    We both worked in the same room to write and test code for the whole competition.

\end{itemize}



\section{Overview}
\medskip
\begin{itemize}

    \item \boldline{Models and techniques tried}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'. Models and techniques should be bolded using '\textbf{}'.
    \item \textbf{Cross validation:} We employed cross validation to analyze the performance of our classifiers.
    \item \textbf{One-Hot Encoding:} One of the first techniques we considered was one-hot encoding for certain inputs. Since some important factors, such as race or marital status, didn't have any numerical meaning, we chose from the beginning to one-hot encode inputs like this.
    \item \textbf{Neural Networks:} One of our first ideas was to train a neural network on the input features. We tried several structures, including a shallow variant with a single hidden layer with 10000 nodes, and a deep variant with five layers having 100-1000 nodes. However, we found these models only were accurate with about 74\% probability.
    \item \textbf{SVM:} One of the techniques we considered was training an SVM on the data, since we expected the output to be somewhat linear in the inputs. However, this was too computationally expensive - the sklearn library took too long to train on the given dataset, so we did not get any model from this classifier.
    \item \textbf{Random Forest/Decision Trees:} We tried decision trees, and then random forest classifiers from  the sklearn library. These classifiers resulted in scores of around 75-76\%.
    \item \textbf{Adaboost:} We used the adaboost technique through the sklearn AdaBoostClassifier (the default model, which we used, was the random forest model) This increased our scores to around 76\%
    \item \textbf{Logistic Regression:} We tried the logistic regression classifier from sklearn. This resulted in low scores (around 73\% out of sample), so we decided to avoid this model.
    \item \textbf{Linear Regression:} We tried the linear regression classifier from sklearn. This also resulted in low scores (around 73\% out of sample), so we decided to avoid this model as well.
    \item \textbf{Gradient Regression:} %TODO what is this and how did it turn out?
    \item \textbf{$k$-Best feature selection:} We tried the scikit SelectKBest method, which transforms the input to remove all but the $k$ best features in terms of univariate statistical dependency of the output on those features. Coupled with the Adaboost classifier, this technique got our scores up to around 77-78\%.


    \end{itemize}

    \item \boldline{Work timeline}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{First Half of first week: Preprocessing/Framework} During this time, we built a framework that would one-hot encode some inputs, and created methods that would allow us to analyze the performance of an arbitrary classifier from the sklearn libraries, and output the results on the competition test data.
    \item \textbf{Second Half of first week: Initial model exploration} During this time, we came up with several different classifiers that we had learned about in class and that were implemented by sklearn. We created files for each of these, tested their performance, and made some initial submissions to the leaderboard.
    \item \textbf{First Half of second week: Dataset manipulation} During this time we used the framework we had built the previous week to manipulate the inputs to the models to get better results. We made a breakthrough in applying the $k$-Best feature selection to prevent overfitting and make the models more accurate.
    \item \textbf{Second Half of second week: Parameter tuning} During this time, we took our best models and analyzed their performance as we varied their input parameters, to hone in on the best possible classifier we could produce.
    \end{itemize}

\end{itemize}



\section{Approach}
\medskip
\begin{itemize}

    \item \boldline{Data processing and manipulation}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{$k$-Best features:}
    We chose to select which features we would use by taking the 250 best features according to their statistical relevance to the voter turnout. We felt that this would be enough to weed out any irrelevant data (such as the survey date or reasons for non-interview) while still keeping most the important factors, such as age and income level.

    \item \textbf{One-Hot Encoding:}
    We chose to One-Hot encode certain important features in the input data. Features that we did this for were "PESEX" (sex), "PTDTRACE" (race), and "GTMETSTA" (metropolitan status), since we felt that these features didn't conform to any intuitive enumeration.

    \end{itemize}

    \item \boldline{Details of models and techniques}
    \begin{itemize}
    % Insert text here. Bullet points can be made using '\item'.
    \item \textbf{Adaboost:} We used an Adaboost classifier with %TODO

    % If you would like to insert a figure, you can just use the following five lines, replacing the image path with your own and the caption with a 1-2 sentence description of what the image is and how it is relevant or useful.
    % \begin{figure}[H]
    % \centering
    % \includegraphics[width=\textwidth]{smiley.png}
    % \caption{Insert caption here.}
    % \end{figure}

    \end{itemize}

\end{itemize}



\section{Model Selection}
\medskip
\begin{itemize}

    \item \boldline{Scoring} \\
    % Insert text here.%TODO

    \item \boldline{Validation and Test} \\
    % Insert text here.%TODO

\end{itemize}



\section{Conclusion}
\medskip
\begin{itemize}

    \item \boldline{Discoveries} \\
    We discovered that, for datasets with a large number of inputs, many of which are irrelevant to the output class, simplifying the input data through a $k$-Best method or some other method that culls useless features can be very helpful to making models behave better. We also learned that random forest classifiers, especially when boosted, can be effective when analyzing polling data.

    \item \boldline{Challenges} \\
    We found that many common classifiers, such as neural networks and SVMs, were ineffective at predicting the voter turnout. We found that it was hard to put the data in an inuitive form to analyze, due to there being a large number of features, some of which were poorly explained in the codebook. We also found that many learning algorithms can be computationally expensive, and so it can take a great deal of time to run through different parameters on algorithms to find the best model.

    \item \boldline{Concluding Remarks} \\
    In the end, our model was successful in correctly classifying over 78\% of the test data. This is something of an underwhelming result, due to the fact that in the training data, 74.5\% of the people surveyed voted, and so even an extremely naive classifier which always predicted that people would vote would probably successfully classify about 75\% of the data. However, since no one in the class was able to get above 79\% on the public leaderboard, we believe that there may not be enough data in the survey to predict to a high degree of accuracy whether someone will vote or not. There are clearly factors at play, such as a person's availability on voting day, which are impossible to predict and which affect turnout. Therefore, we believe that getting a 78\% classification rate from the data we were given constitutes a success.

\end{itemize}



\end{document}
